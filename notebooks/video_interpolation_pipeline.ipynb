{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d515bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import wandb  # Para tracking de experimentos\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed9d610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ec3d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vimeo90kDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para Vimeo-90k Triplet\n",
    "    Carrega triplas de frames para treinamento de interpolação\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        is_train: bool = True,\n",
    "        transform=None,\n",
    "        crop_size: Tuple[int, int] = (256, 256)\n",
    "    ):\n",
    "        print(\"Initializing Vimeo90kDataset...\")\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.is_train = is_train\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "        print(f\"Data directory: {self.data_dir}\")\n",
    "        # Carregar lista de sequências\n",
    "        list_file = 'tri_trainlist.txt' if is_train else 'tri_testlist.txt'\n",
    "        list_path = self.data_dir / list_file\n",
    "        \n",
    "        print(f\"List file path: {list_path}\")\n",
    "\n",
    "        with open(list_path, 'r') as f:\n",
    "            self.triplets = [line.strip() for line in f.readlines()]\n",
    "            print(f\"First 5 triplets: {self.triplets[:5]}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.triplets)} triplets for {'training' if is_train else 'testing'}\")\n",
    "        \n",
    "        # Transformações\n",
    "        if transform is None:\n",
    "            if is_train:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.RandomCrop(crop_size),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomVerticalFlip(p=0.5),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.CenterCrop(crop_size),\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Triplet path: 00001/0001\n",
    "        triplet_path = self.triplets[idx]\n",
    "        base_path = self.data_dir / 'sequences' / triplet_path\n",
    "        \n",
    "        # Carregar os 3 frames\n",
    "        frame1 = self._load_image(base_path / 'im1.png')\n",
    "        frame2 = self._load_image(base_path / 'im2.png')  # Ground truth (meio)\n",
    "        frame3 = self._load_image(base_path / 'im3.png')\n",
    "        \n",
    "        # Stack para aplicar mesmas transformações\n",
    "        frames = torch.cat([frame1, frame2, frame3], dim=0)\n",
    "        \n",
    "        # Aplicar transformações\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "        \n",
    "        # Separar frames novamente\n",
    "        frame1 = frames[:3, :, :]\n",
    "        frame2 = frames[3:6, :, :]\n",
    "        frame3 = frames[6:9, :, :]\n",
    "        \n",
    "        return {\n",
    "            'frame1': frame1,\n",
    "            'frame2': frame2,  # Ground truth\n",
    "            'frame3': frame3,\n",
    "            'triplet_path': triplet_path\n",
    "        }\n",
    "    \n",
    "    def _load_image(self, path: Path) -> torch.Tensor:\n",
    "        \"\"\"Carregar imagem e converter para tensor\"\"\"\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img = transforms.ToTensor()(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72f236b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class UNetInterpolator(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net para Frame Interpolation\n",
    "    Input: frame1 + frame3 (6 canais)\n",
    "    Output: frame2 interpolado (3 canais)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=6, out_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_channels, 64)\n",
    "        self.enc2 = ConvBlock(64, 128)\n",
    "        self.enc3 = ConvBlock(128, 256)\n",
    "        self.enc4 = ConvBlock(256, 512)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.dec4 = ConvBlock(1024, 512)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = ConvBlock(512, 256)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(256, 128)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(128, 64)\n",
    "        \n",
    "        # Output\n",
    "        self.out = nn.Conv2d(64, out_channels, 1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, frame1, frame3):\n",
    "        # Concatenar frames de entrada\n",
    "        x = torch.cat([frame1, frame3], dim=1)\n",
    "        \n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        x = self.pool(enc1)\n",
    "        \n",
    "        enc2 = self.enc2(x)\n",
    "        x = self.pool(enc2)\n",
    "        \n",
    "        enc3 = self.enc3(x)\n",
    "        x = self.pool(enc3)\n",
    "        \n",
    "        enc4 = self.enc4(x)\n",
    "        x = self.pool(enc4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.upconv4(x)\n",
    "        x = torch.cat([x, enc4], dim=1)\n",
    "        x = self.dec4(x)\n",
    "        \n",
    "        x = self.upconv3(x)\n",
    "        x = torch.cat([x, enc3], dim=1)\n",
    "        x = self.dec3(x)\n",
    "        \n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat([x, enc2], dim=1)\n",
    "        x = self.dec2(x)\n",
    "        \n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat([x, enc1], dim=1)\n",
    "        x = self.dec1(x)\n",
    "        \n",
    "        # Output\n",
    "        out = self.out(x)\n",
    "        out = torch.sigmoid(out)  # Valores entre 0 e 1\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Combinação de múltiplas losses para melhor qualidade:\n",
    "- L1 Loss: Reconstrução pixel-wise\n",
    "- Perceptual Loss: Similaridade em features de alto nível\n",
    "- SSIM Loss: Similaridade estrutural\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    import lpips\n",
    "    LPIPS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LPIPS_AVAILABLE = False\n",
    "    print(\"Warning: lpips not available. Install with: pip install lpips\")\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "        # Perceptual loss usando LPIPS\n",
    "        if LPIPS_AVAILABLE:\n",
    "            self.lpips_loss = lpips.LPIPS(net='alex').to(device)\n",
    "        else:\n",
    "            self.lpips_loss = None\n",
    "        \n",
    "        # Pesos das losses\n",
    "        self.w_l1 = 1.0\n",
    "        self.w_perceptual = 0.1 if LPIPS_AVAILABLE else 0.0\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # L1 Loss\n",
    "        loss_l1 = self.l1_loss(pred, target)\n",
    "        \n",
    "        total_loss = self.w_l1 * loss_l1\n",
    "        \n",
    "        # Perceptual Loss\n",
    "        if self.lpips_loss is not None:\n",
    "            # LPIPS espera valores em [-1, 1]\n",
    "            pred_norm = pred * 2 - 1\n",
    "            target_norm = target * 2 - 1\n",
    "            loss_perceptual = self.lpips_loss(pred_norm, target_norm).mean()\n",
    "            total_loss += self.w_perceptual * loss_perceptual\n",
    "        \n",
    "        return total_loss, {\n",
    "            'l1': loss_l1.item(),\n",
    "            'perceptual': loss_perceptual.item() if self.lpips_loss else 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4d13346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(pred, target):\n",
    "    \"\"\"Calculate Peak Signal-to-Noise Ratio\"\"\"\n",
    "    mse = torch.mean((pred - target) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    max_pixel = 1.0\n",
    "    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))\n",
    "    return psnr.item()\n",
    "\n",
    "def calculate_ssim(pred, target):\n",
    "    \"\"\"Calculate Structural Similarity Index (simplificado)\"\"\"\n",
    "    # Para SSIM completo, use pytorch-msssim\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "    \n",
    "    mu_pred = torch.mean(pred)\n",
    "    mu_target = torch.mean(target)\n",
    "    \n",
    "    sigma_pred = torch.var(pred)\n",
    "    sigma_target = torch.var(target)\n",
    "    sigma_pred_target = torch.mean((pred - mu_pred) * (target - mu_target))\n",
    "    \n",
    "    ssim = ((2 * mu_pred * mu_target + C1) * (2 * sigma_pred_target + C2)) / \\\n",
    "           ((mu_pred ** 2 + mu_target ** 2 + C1) * (sigma_pred + sigma_target + C2))\n",
    "    \n",
    "    return ssim.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60f40d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device,\n",
    "        config\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        \n",
    "        self.best_val_loss = float('inf')\n",
    "        self.start_epoch = 0\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(self.train_loader, desc=f'Epoch {epoch}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            frame1 = batch['frame1'].to(self.device)\n",
    "            frame2 = batch['frame2'].to(self.device)  # Ground truth\n",
    "            frame3 = batch['frame3'].to(self.device)\n",
    "            \n",
    "            # Forward\n",
    "            pred_frame2 = self.model(frame1, frame3)\n",
    "            \n",
    "            # Loss\n",
    "            loss, loss_dict = self.criterion(pred_frame2, frame2)\n",
    "            \n",
    "            # Backward\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'l1': f'{loss_dict[\"l1\"]:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Log to wandb\n",
    "            if self.config.get('use_wandb', False):\n",
    "                wandb.log({\n",
    "                    'train/loss': loss.item(),\n",
    "                    'train/l1_loss': loss_dict['l1'],\n",
    "                    'train/perceptual_loss': loss_dict['perceptual'],\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        return avg_loss\n",
    "    \n",
    "    def validate(self, epoch):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_psnr = 0\n",
    "        total_ssim = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc='Validation'):\n",
    "                frame1 = batch['frame1'].to(self.device)\n",
    "                frame2 = batch['frame2'].to(self.device)\n",
    "                frame3 = batch['frame3'].to(self.device)\n",
    "                \n",
    "                pred_frame2 = self.model(frame1, frame3)\n",
    "                \n",
    "                loss, _ = self.criterion(pred_frame2, frame2)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Calcular métricas\n",
    "                psnr = calculate_psnr(pred_frame2, frame2)\n",
    "                ssim = calculate_ssim(pred_frame2, frame2)\n",
    "                \n",
    "                total_psnr += psnr\n",
    "                total_ssim += ssim\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        avg_psnr = total_psnr / len(self.val_loader)\n",
    "        avg_ssim = total_ssim / len(self.val_loader)\n",
    "        \n",
    "        print(f'\\nValidation - Loss: {avg_loss:.4f}, PSNR: {avg_psnr:.2f}, SSIM: {avg_ssim:.4f}')\n",
    "        \n",
    "        if self.config.get('use_wandb', False):\n",
    "            wandb.log({\n",
    "                'val/loss': avg_loss,\n",
    "                'val/psnr': avg_psnr,\n",
    "                'val/ssim': avg_ssim,\n",
    "                'epoch': epoch\n",
    "            })\n",
    "        \n",
    "        return avg_loss, avg_psnr, avg_ssim\n",
    "    \n",
    "    def save_checkpoint(self, epoch, val_loss, is_best=False):\n",
    "        checkpoint_dir = Path(self.config['paths']['finetuned_dir'])\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        if epoch % self.config['training']['checkpoint_freq'] == 0:\n",
    "            path = checkpoint_dir / f'checkpoint_epoch_{epoch}.pt'\n",
    "            torch.save(checkpoint, path)\n",
    "            print(f'Checkpoint saved: {path}')\n",
    "        \n",
    "        # Save best model\n",
    "        if is_best:\n",
    "            path = checkpoint_dir / 'best_model.pt'\n",
    "            torch.save(checkpoint, path)\n",
    "            print(f'Best model saved: {path}')\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        print(f'Starting training for {num_epochs} epochs...')\n",
    "        \n",
    "        for epoch in range(self.start_epoch, num_epochs):\n",
    "            # Train\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            print(f'Epoch {epoch} - Train Loss: {train_loss:.4f}')\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_psnr, val_ssim = self.validate(epoch)\n",
    "            \n",
    "            # Save checkpoint\n",
    "            is_best = val_loss < self.best_val_loss\n",
    "            if is_best:\n",
    "                self.best_val_loss = val_loss\n",
    "            \n",
    "            self.save_checkpoint(epoch, val_loss, is_best)\n",
    "        \n",
    "        print('Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6364e3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vimeo Dataset Directory: data\\vimeo_triplet\n",
      "Loading datasets...\n",
      "Initializing Vimeo90kDataset...\n",
      "Data directory: data\\vimeo_triplet\n",
      "List file path: data\\vimeo_triplet\\tri_trainlist.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\vimeo_triplet\\\\tri_trainlist.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading datasets...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mbreakpoint\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m train_dataset = \u001b[43mVimeo90kDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVIMEO_DATA_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcrop_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m val_dataset = Vimeo90kDataset(\n\u001b[32m     19\u001b[39m     data_dir=VIMEO_DATA_DIR,\n\u001b[32m     20\u001b[39m     is_train=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     21\u001b[39m     crop_size=(\u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m)\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Criar dataloaders\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mVimeo90kDataset.__init__\u001b[39m\u001b[34m(self, data_dir, is_train, transform, crop_size)\u001b[39m\n\u001b[32m     21\u001b[39m list_path = \u001b[38;5;28mself\u001b[39m.data_dir / list_file\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mList file path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlist_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlist_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mself\u001b[39m.triplets = [line.strip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f.readlines()]\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFirst 5 triplets: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.triplets[:\u001b[32m5\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UC 14\\ia_senac_uc14_frame_generation_project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data\\\\vimeo_triplet\\\\tri_trainlist.txt'"
     ]
    }
   ],
   "source": [
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Atualizar paths para o dataset Vimeo-90k\n",
    "VIMEO_DATA_DIR = Path(config['paths']['data_dir']) / 'vimeo_triplet'\n",
    "\n",
    "print(\"Vimeo Dataset Directory:\", VIMEO_DATA_DIR)\n",
    "\n",
    "# Criar datasets\n",
    "print(\"Loading datasets...\")\n",
    "breakpoint()\n",
    "train_dataset = Vimeo90kDataset(\n",
    "    data_dir=VIMEO_DATA_DIR,\n",
    "    is_train=True,\n",
    "    crop_size=(256, 256)\n",
    ")\n",
    "\n",
    "val_dataset = Vimeo90kDataset(\n",
    "    data_dir=VIMEO_DATA_DIR,\n",
    "    is_train=False,\n",
    "    crop_size=(256, 256)\n",
    ")\n",
    "\n",
    "# Criar dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['processing']['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['processing']['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Criar modelo\n",
    "model = UNetInterpolator(in_channels=6, out_channels=3).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Optimizer e Loss\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config['training']['learning_rate'],\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "criterion = CombinedLoss(device=device)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Inicializar Weights & Biases (opcional)\n",
    "USE_WANDB = False  # Mudar para True se quiser usar wandb\n",
    "if USE_WANDB:\n",
    "    wandb.init(\n",
    "        project='video-interpolation',\n",
    "        config=config,\n",
    "        name=f'unet-vimeo90k-{config[\"training\"][\"learning_rate\"]}'\n",
    "    )\n",
    "\n",
    "config['use_wandb'] = USE_WANDB\n",
    "\n",
    "# Criar trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Iniciar treinamento\n",
    "trainer.train(num_epochs=config['training']['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cecc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(model, dataset, num_samples=5):\n",
    "    \"\"\"Visualizar resultados do modelo\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            sample = dataset[i]\n",
    "            \n",
    "            frame1 = sample['frame1'].unsqueeze(0).to(device)\n",
    "            frame2_gt = sample['frame2']\n",
    "            frame3 = sample['frame3'].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Predição\n",
    "            frame2_pred = model(frame1, frame3).cpu().squeeze(0)\n",
    "            \n",
    "            # Converter para numpy\n",
    "            f1 = frame1.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "            f2_gt = frame2_gt.permute(1, 2, 0).numpy()\n",
    "            f2_pred = frame2_pred.permute(1, 2, 0).numpy()\n",
    "            f3 = frame3.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "            \n",
    "            # Plot\n",
    "            axes[i, 0].imshow(f1)\n",
    "            axes[i, 0].set_title('Frame 1')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(f2_gt)\n",
    "            axes[i, 1].set_title('Frame 2 (GT)')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(f2_pred)\n",
    "            axes[i, 2].set_title('Frame 2 (Pred)')\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "            axes[i, 3].imshow(f3)\n",
    "            axes[i, 3].set_title('Frame 3')\n",
    "            axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/output/interpolation_results.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar resultados\n",
    "visualize_results(model, val_dataset, num_samples=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia-senac-uc14-frame-generation-project-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
