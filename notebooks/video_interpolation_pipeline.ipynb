{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d515bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import wandb  # Para tracking de experimentos\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vimeo90kDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para Vimeo-90k Triplet\n",
    "    Carrega triplas de frames para treinamento de interpolação\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        is_train: bool = True,\n",
    "        transform=None,\n",
    "        crop_size: Tuple[int, int] = (256, 256),\n",
    "        cache=True  # NOVO: ativar cache\n",
    "    ):\n",
    "        print(\"Initializing Vimeo90kDataset...\")\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.is_train = is_train\n",
    "        self.crop_size = crop_size\n",
    "        self.cache = cache\n",
    "        self.image_cache = {}  # NOVO: dicionário de cache\n",
    "\n",
    "        print(f\"Data directory: {self.data_dir}\")\n",
    "        # Carregar lista de sequências\n",
    "        list_file = 'tri_trainlist.txt' if is_train else 'tri_testlist.txt'\n",
    "        list_path = self.data_dir / list_file\n",
    "        \n",
    "        print(f\"List file path: {list_path}\")\n",
    "\n",
    "        with open(list_path, 'r') as f:\n",
    "            self.triplets = [line.strip() for line in f.readlines()]\n",
    "            print(f\"First 5 triplets: {self.triplets[:5]}\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.triplets)} triplets for {'training' if is_train else 'testing'}\")\n",
    "        \n",
    "        # Transformações\n",
    "        if transform is None:\n",
    "            if is_train:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.RandomCrop(crop_size),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomVerticalFlip(p=0.5),\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.CenterCrop(crop_size),\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Triplet path: 00001/0001\n",
    "        triplet_path = self.triplets[idx]\n",
    "        base_path = self.data_dir / 'sequences' / triplet_path\n",
    "        \n",
    "        # Carregar os 3 frames (com cache)\n",
    "        frame1 = self._load_image(base_path / 'im1.png')\n",
    "        frame2 = self._load_image(base_path / 'im2.png')  # Ground truth (meio)\n",
    "        frame3 = self._load_image(base_path / 'im3.png')\n",
    "        \n",
    "        # Stack para aplicar mesmas transformações\n",
    "        frames = torch.cat([frame1, frame2, frame3], dim=0)\n",
    "        \n",
    "        # Aplicar transformações\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "        \n",
    "        # Separar frames novamente\n",
    "        frame1 = frames[:3, :, :]\n",
    "        frame2 = frames[3:6, :, :]\n",
    "        frame3 = frames[6:9, :, :]\n",
    "        \n",
    "        return {\n",
    "            'frame1': frame1,\n",
    "            'frame2': frame2,  # Ground truth\n",
    "            'frame3': frame3,\n",
    "            'triplet_path': triplet_path\n",
    "        }\n",
    "    \n",
    "    def _load_image(self, path: Path) -> torch.Tensor:\n",
    "        \"\"\"Carregar imagem e converter para tensor (com cache)\"\"\"\n",
    "        path_str = str(path)\n",
    "        \n",
    "        # Se está em cache e cache está ativado, retornar do cache\n",
    "        if self.cache and path_str in self.image_cache:\n",
    "            return self.image_cache[path_str].clone()\n",
    "        \n",
    "        # Senão, carregar do disco\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        \n",
    "        # Guardar no cache se ativado\n",
    "        if self.cache:\n",
    "            self.image_cache[path_str] = img_tensor\n",
    "        \n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f236b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class UNetInterpolator(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net para Frame Interpolation\n",
    "    Input: frame1 + frame3 (6 canais)\n",
    "    Output: frame2 interpolado (3 canais)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=6, out_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(in_channels, 64)\n",
    "        self.enc2 = ConvBlock(64, 128)\n",
    "        self.enc3 = ConvBlock(128, 256)\n",
    "        self.enc4 = ConvBlock(256, 512)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.dec4 = ConvBlock(1024, 512)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = ConvBlock(512, 256)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(256, 128)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(128, 64)\n",
    "        \n",
    "        # Output\n",
    "        self.out = nn.Conv2d(64, out_channels, 1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, frame1, frame3):\n",
    "        # Concatenar frames de entrada\n",
    "        x = torch.cat([frame1, frame3], dim=1)\n",
    "        \n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        x = self.pool(enc1)\n",
    "        \n",
    "        enc2 = self.enc2(x)\n",
    "        x = self.pool(enc2)\n",
    "        \n",
    "        enc3 = self.enc3(x)\n",
    "        x = self.pool(enc3)\n",
    "        \n",
    "        enc4 = self.enc4(x)\n",
    "        x = self.pool(enc4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.upconv4(x)\n",
    "        x = torch.cat([x, enc4], dim=1)\n",
    "        x = self.dec4(x)\n",
    "        \n",
    "        x = self.upconv3(x)\n",
    "        x = torch.cat([x, enc3], dim=1)\n",
    "        x = self.dec3(x)\n",
    "        \n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat([x, enc2], dim=1)\n",
    "        x = self.dec2(x)\n",
    "        \n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat([x, enc1], dim=1)\n",
    "        x = self.dec1(x)\n",
    "        \n",
    "        # Output\n",
    "        out = self.out(x)\n",
    "        out = torch.sigmoid(out)  # Valores entre 0 e 1\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Combinação de múltiplas losses para melhor qualidade:\n",
    "- L1 Loss: Reconstrução pixel-wise\n",
    "- Perceptual Loss: Similaridade em features de alto nível\n",
    "- SSIM Loss: Similaridade estrutural\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    import lpips\n",
    "    LPIPS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LPIPS_AVAILABLE = False\n",
    "    print(\"Warning: lpips not available. Install with: pip install lpips\")\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "        # Perceptual loss usando LPIPS\n",
    "        if LPIPS_AVAILABLE:\n",
    "            self.lpips_loss = lpips.LPIPS(net='alex').to(device)\n",
    "        else:\n",
    "            self.lpips_loss = None\n",
    "        \n",
    "        # Pesos das losses\n",
    "        self.w_l1 = 1.0\n",
    "        self.w_perceptual = 0.1 if LPIPS_AVAILABLE else 0.0\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # L1 Loss\n",
    "        loss_l1 = self.l1_loss(pred, target)\n",
    "        \n",
    "        total_loss = self.w_l1 * loss_l1\n",
    "        \n",
    "        # Perceptual Loss\n",
    "        if self.lpips_loss is not None:\n",
    "            # LPIPS espera valores em [-1, 1]\n",
    "            pred_norm = pred * 2 - 1\n",
    "            target_norm = target * 2 - 1\n",
    "            loss_perceptual = self.lpips_loss(pred_norm, target_norm).mean()\n",
    "            total_loss += self.w_perceptual * loss_perceptual\n",
    "        \n",
    "        return total_loss, {\n",
    "            'l1': loss_l1.item(),\n",
    "            'perceptual': loss_perceptual.item() if self.lpips_loss else 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d13346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(pred, target):\n",
    "    \"\"\"Calculate Peak Signal-to-Noise Ratio\"\"\"\n",
    "    mse = torch.mean((pred - target) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    max_pixel = 1.0\n",
    "    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))\n",
    "    return psnr.item()\n",
    "\n",
    "def calculate_ssim(pred, target):\n",
    "    \"\"\"Calculate Structural Similarity Index (simplificado)\"\"\"\n",
    "    # Para SSIM completo, use pytorch-msssim\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "    \n",
    "    mu_pred = torch.mean(pred)\n",
    "    mu_target = torch.mean(target)\n",
    "    \n",
    "    sigma_pred = torch.var(pred)\n",
    "    sigma_target = torch.var(target)\n",
    "    sigma_pred_target = torch.mean((pred - mu_pred) * (target - mu_target))\n",
    "    \n",
    "    ssim = ((2 * mu_pred * mu_target + C1) * (2 * sigma_pred_target + C2)) / \\\n",
    "           ((mu_pred ** 2 + mu_target ** 2 + C1) * (sigma_pred + sigma_target + C2))\n",
    "    \n",
    "    return ssim.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f40d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device,\n",
    "        config\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        \n",
    "        self.best_val_loss = float('inf')\n",
    "        self.start_epoch = 0\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(self.train_loader, desc=f'Epoch {epoch}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            frame1 = batch['frame1'].to(self.device)\n",
    "            frame2 = batch['frame2'].to(self.device)  # Ground truth\n",
    "            frame3 = batch['frame3'].to(self.device)\n",
    "            \n",
    "            # Forward\n",
    "            pred_frame2 = self.model(frame1, frame3)\n",
    "            \n",
    "            # Loss\n",
    "            loss, loss_dict = self.criterion(pred_frame2, frame2)\n",
    "            \n",
    "            # Backward\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'l1': f'{loss_dict[\"l1\"]:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Log to wandb\n",
    "            if self.config.get('use_wandb', False):\n",
    "                wandb.log({\n",
    "                    'train/loss': loss.item(),\n",
    "                    'train/l1_loss': loss_dict['l1'],\n",
    "                    'train/perceptual_loss': loss_dict['perceptual'],\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        return avg_loss\n",
    "    \n",
    "    def validate(self, epoch):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_psnr = 0\n",
    "        total_ssim = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc='Validation'):\n",
    "                frame1 = batch['frame1'].to(self.device)\n",
    "                frame2 = batch['frame2'].to(self.device)\n",
    "                frame3 = batch['frame3'].to(self.device)\n",
    "                \n",
    "                pred_frame2 = self.model(frame1, frame3)\n",
    "                \n",
    "                loss, _ = self.criterion(pred_frame2, frame2)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Calcular métricas\n",
    "                psnr = calculate_psnr(pred_frame2, frame2)\n",
    "                ssim = calculate_ssim(pred_frame2, frame2)\n",
    "                \n",
    "                total_psnr += psnr\n",
    "                total_ssim += ssim\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        avg_psnr = total_psnr / len(self.val_loader)\n",
    "        avg_ssim = total_ssim / len(self.val_loader)\n",
    "        \n",
    "        print(f'\\nValidation - Loss: {avg_loss:.4f}, PSNR: {avg_psnr:.2f}, SSIM: {avg_ssim:.4f}')\n",
    "        \n",
    "        if self.config.get('use_wandb', False):\n",
    "            wandb.log({\n",
    "                'val/loss': avg_loss,\n",
    "                'val/psnr': avg_psnr,\n",
    "                'val/ssim': avg_ssim,\n",
    "                'epoch': epoch\n",
    "            })\n",
    "        \n",
    "        return avg_loss, avg_psnr, avg_ssim\n",
    "    \n",
    "    def save_checkpoint(self, epoch, val_loss, is_best=False):\n",
    "        checkpoint_dir = Path(self.config['paths']['finetuned_dir'])\n",
    "        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        if epoch % self.config['training']['checkpoint_freq'] == 0:\n",
    "            path = checkpoint_dir / f'checkpoint_epoch_{epoch}.pt'\n",
    "            torch.save(checkpoint, path)\n",
    "            print(f'Checkpoint saved: {path}')\n",
    "        \n",
    "        # Save best model\n",
    "        if is_best:\n",
    "            path = checkpoint_dir / 'best_model.pt'\n",
    "            torch.save(checkpoint, path)\n",
    "            print(f'Best model saved: {path}')\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        print(f'Starting training for {num_epochs} epochs...')\n",
    "        \n",
    "        for epoch in range(self.start_epoch, num_epochs):\n",
    "            # Train\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            print(f'Epoch {epoch} - Train Loss: {train_loss:.4f}')\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_psnr, val_ssim = self.validate(epoch)\n",
    "            \n",
    "            # Save checkpoint\n",
    "            is_best = val_loss < self.best_val_loss\n",
    "            if is_best:\n",
    "                self.best_val_loss = val_loss\n",
    "            \n",
    "            self.save_checkpoint(epoch, val_loss, is_best)\n",
    "        \n",
    "        print('Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "# Tenta achar config.yaml no cwd; se não, no pai\n",
    "candidates = [Path.cwd(), Path.cwd().parent]\n",
    "PROJECT_ROOT = None\n",
    "for c in candidates:\n",
    "    if (c / 'config.yaml').exists():\n",
    "        PROJECT_ROOT = c\n",
    "        break\n",
    "if PROJECT_ROOT is None:\n",
    "    raise FileNotFoundError(\"config.yaml não encontrado; ajuste PROJECT_ROOT manualmente\")\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "\n",
    "with open(PROJECT_ROOT / 'config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Caminho absoluto para o Vimeo-90k\n",
    "VIMEO_DATA_DIR = PROJECT_ROOT / config['paths']['data_dir'] / 'vimeo_triplet'\n",
    "print(\"Vimeo Dataset Directory:\", VIMEO_DATA_DIR)\n",
    "\n",
    "# Sanidade\n",
    "assert (VIMEO_DATA_DIR / 'tri_trainlist.txt').exists(), \"tri_trainlist.txt não encontrado\"\n",
    "\n",
    "# Criar datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset_full = Vimeo90kDataset(\n",
    "    data_dir=VIMEO_DATA_DIR,\n",
    "    is_train=True,\n",
    "    crop_size=(256, 256),\n",
    "    cache=False\n",
    ")\n",
    "val_dataset_full = Vimeo90kDataset(\n",
    "    data_dir=VIMEO_DATA_DIR,\n",
    "    is_train=False,\n",
    "    crop_size=(256, 256),\n",
    "    cache=False\n",
    ")\n",
    "\n",
    "# TESTE RÁPIDO: Usar apenas 15% dos dados\n",
    "SUBSET_RATIO = 0.15  # 15% dos dados\n",
    "train_size = int(len(train_dataset_full) * SUBSET_RATIO)\n",
    "val_size = int(len(val_dataset_full) * SUBSET_RATIO)\n",
    "\n",
    "train_indices = random.sample(range(len(train_dataset_full)), train_size)\n",
    "val_indices = random.sample(range(len(val_dataset_full)), val_size)\n",
    "train_dataset = Subset(train_dataset_full, train_indices)\n",
    "val_dataset = Subset(val_dataset_full, val_indices)\n",
    "\n",
    "print(f\"Using {SUBSET_RATIO*100}% subset: {len(train_dataset)} train, {len(val_dataset)} val samples\")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,  # Reduzido para 4\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # IMPORTANTE: 0 para notebooks no Windows\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Criar modelo\n",
    "model = UNetInterpolator(in_channels=6, out_channels=3).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Optimizer e Loss\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config['training']['learning_rate'],\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "criterion = CombinedLoss(device=device)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "# Inicializar Weights & Biases (opcional)\n",
    "USE_WANDB = False\n",
    "if USE_WANDB:\n",
    "    wandb.init(\n",
    "        project='video-interpolation',\n",
    "        config=config,\n",
    "        name=f'unet-vimeo90k-{config[\"training\"][\"learning_rate\"]}'\n",
    "    )\n",
    "\n",
    "config['use_wandb'] = USE_WANDB\n",
    "\n",
    "# Criar trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Teste rápido: quanto tempo leva para iterar 5 batches?\n",
    "import time\n",
    "print(\"Testing data loading speed...\")\n",
    "start = time.time()\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(f\"Batch {i}: frames shape = {batch['frame1'].shape}\")\n",
    "elapsed = time.time() - start\n",
    "print(f\"5 batches levaram {elapsed:.2f}s ({elapsed/5:.2f}s por batch)\")\n",
    "\n",
    "# Iniciar treinamento\n",
    "trainer.train(num_epochs=config['training']['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cecc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(model, dataset, num_samples=5):\n",
    "    \"\"\"Visualizar resultados do modelo\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            sample = dataset[i]\n",
    "            \n",
    "            frame1 = sample['frame1'].unsqueeze(0).to(device)\n",
    "            frame2_gt = sample['frame2']\n",
    "            frame3 = sample['frame3'].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Predição\n",
    "            frame2_pred = model(frame1, frame3).cpu().squeeze(0)\n",
    "            \n",
    "            # Converter para numpy\n",
    "            f1 = frame1.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "            f2_gt = frame2_gt.permute(1, 2, 0).numpy()\n",
    "            f2_pred = frame2_pred.permute(1, 2, 0).numpy()\n",
    "            f3 = frame3.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "            \n",
    "            # Plot\n",
    "            axes[i, 0].imshow(f1)\n",
    "            axes[i, 0].set_title('Frame 1')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(f2_gt)\n",
    "            axes[i, 1].set_title('Frame 2 (GT)')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(f2_pred)\n",
    "            axes[i, 2].set_title('Frame 2 (Pred)')\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "            axes[i, 3].imshow(f3)\n",
    "            axes[i, 3].set_title('Frame 3')\n",
    "            axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/output/interpolation_results.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar resultados\n",
    "visualize_results(model, val_dataset, num_samples=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia-senac-uc14-frame-generation-project-awmm63cb-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
