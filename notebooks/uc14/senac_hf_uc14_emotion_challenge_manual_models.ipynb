{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2f12af",
   "metadata": {},
   "source": [
    "# Desafio Kaggle ‚Äî Classifica√ß√£o de Emo√ß√µes (Modelos Manuais)\n",
    "\n",
    "> Notebook organizado para apresenta√ß√£o do trabalho da UC14, com modelos constru√≠dos manualmente (RNN/LSTM) em PyTorch + Hugging Face Trainer.\n",
    "\n",
    "## Objetivo\n",
    "- Treinar e avaliar um classificador de emo√ß√µes sem usar backbone pr√©-treinado de linguagem.\n",
    "- Comparar arquiteturas manuais recorrentes (RNN e LSTM).\n",
    "- Gerar submiss√£o Kaggle no formato esperado.\n",
    "\n",
    "## Estrutura\n",
    "1. Setup e imports\n",
    "2. Defini√ß√£o do modelo manual (RNN/LSTM)\n",
    "3. Pipeline de dados\n",
    "4. Treinamento e m√©tricas\n",
    "5. Avalia√ß√£o, infer√™ncia e submiss√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1820639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import csv\n",
    "\n",
    "def get_best_available_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = get_best_available_device()\n",
    "print(f\"Device selecionado: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b5f9d",
   "metadata": {},
   "source": [
    "## 1) Defini√ß√£o do Modelo Manual (RNN ou LSTM)\n",
    "\n",
    "Escolha a arquitetura no par√¢metro `MODEL_TYPE` (`\"rnn\"` ou `\"lstm\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1286fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Arquiteturas manuais compat√≠veis com Hugging Face\n",
    "\n",
    "MODEL_TYPE = \"lstm\"  # op√ß√µes: \"rnn\" ou \"lstm\"\n",
    "\n",
    "class RNNConfig(PretrainedConfig):\n",
    "    model_type = \"custom_raw_rnn\"\n",
    "\n",
    "    def __init__(self, vocab_size=30522, embedding_dim=64, hidden_dim=128, n_classes=6, dropout=0.2, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = dropout\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class LSTMConfig(PretrainedConfig):\n",
    "    model_type = \"custom_raw_lstm\"\n",
    "\n",
    "    def __init__(self, vocab_size=30522, embedding_dim=64, hidden_dim=128, n_classes=6, dropout=0.2, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = dropout\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "class VanillaRNNLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h_t = x.new_zeros((batch_size, self.hidden_size))\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            next_h = self.activation(self.i2h(x_t) + self.h2h(h_t))\n",
    "            if attention_mask is not None:\n",
    "                mask_t = attention_mask[:, t].unsqueeze(1).type_as(next_h)\n",
    "                h_t = mask_t * next_h + (1.0 - mask_t) * h_t\n",
    "            else:\n",
    "                h_t = next_h\n",
    "            outputs.append(h_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.x2h = nn.Linear(input_size, 4 * hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, 4 * hidden_size)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h_t = x.new_zeros((batch_size, self.hidden_size))\n",
    "        c_t = x.new_zeros((batch_size, self.hidden_size))\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            gates = self.x2h(x_t) + self.h2h(h_t)\n",
    "            i_gate, f_gate, g_gate, o_gate = gates.chunk(4, dim=1)\n",
    "\n",
    "            i_gate = torch.sigmoid(i_gate)\n",
    "            f_gate = torch.sigmoid(f_gate)\n",
    "            g_gate = torch.tanh(g_gate)\n",
    "            o_gate = torch.sigmoid(o_gate)\n",
    "\n",
    "            next_c = f_gate * c_t + i_gate * g_gate\n",
    "            next_h = o_gate * torch.tanh(next_c)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                mask_t = attention_mask[:, t].unsqueeze(1).type_as(next_h)\n",
    "                h_t = mask_t * next_h + (1.0 - mask_t) * h_t\n",
    "                c_t = mask_t * next_c + (1.0 - mask_t) * c_t\n",
    "            else:\n",
    "                h_t = next_h\n",
    "                c_t = next_c\n",
    "            outputs.append(h_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "class TextClassificationRawRNN(PreTrainedModel):\n",
    "    config_class = RNNConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.embedding_dim, padding_idx=0)\n",
    "        self.rnn_block = VanillaRNNLayer(config.embedding_dim, config.hidden_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_dim, config.n_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.all_tied_weights_keys = []\n",
    "        self._tied_weights_keys = []\n",
    "        self.post_init()\n",
    "\n",
    "    @property\n",
    "    def dummy_inputs(self):\n",
    "        return {\"input_ids\": torch.tensor([[0, 1]]), \"attention_mask\": torch.tensor([[1, 1]])}\n",
    "\n",
    "    def _check_and_adjust_experts_implementation(self, experts_implementation):\n",
    "        return experts_implementation\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        x = self.embedding(input_ids)\n",
    "        rnn_output = self.rnn_block(x, attention_mask=attention_mask)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = input_ids.shape[0]\n",
    "            last_hidden_states = rnn_output[torch.arange(batch_size, device=x.device), sequence_lengths]\n",
    "        else:\n",
    "            last_hidden_states = rnn_output[:, -1, :]\n",
    "\n",
    "        logits = self.classifier(self.dropout(last_hidden_states))\n",
    "        loss = self.loss_fn(logits, labels) if labels is not None else None\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "class TextClassificationRawLSTM(PreTrainedModel):\n",
    "    config_class = LSTMConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.embedding_dim, padding_idx=0)\n",
    "        self.lstm_block = LSTMLayer(config.embedding_dim, config.hidden_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_dim, config.n_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.all_tied_weights_keys = []\n",
    "        self._tied_weights_keys = []\n",
    "        self.post_init()\n",
    "\n",
    "    @property\n",
    "    def dummy_inputs(self):\n",
    "        return {\"input_ids\": torch.tensor([[0, 1]]), \"attention_mask\": torch.tensor([[1, 1]])}\n",
    "\n",
    "    def _check_and_adjust_experts_implementation(self, experts_implementation):\n",
    "        return experts_implementation\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        x = self.embedding(input_ids)\n",
    "        lstm_output = self.lstm_block(x, attention_mask=attention_mask)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = input_ids.shape[0]\n",
    "            last_hidden_states = lstm_output[torch.arange(batch_size, device=x.device), sequence_lengths]\n",
    "        else:\n",
    "            last_hidden_states = lstm_output[:, -1, :]\n",
    "\n",
    "        logits = self.classifier(self.dropout(last_hidden_states))\n",
    "        loss = self.loss_fn(logits, labels) if labels is not None else None\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "def build_manual_model(model_type, vocab_size, n_classes, embedding_dim=64, hidden_dim=128, dropout=0.2):\n",
    "    if model_type.lower() == \"rnn\":\n",
    "        config = RNNConfig(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim, n_classes=n_classes, dropout=dropout)\n",
    "        return TextClassificationRawRNN(config)\n",
    "\n",
    "    if model_type.lower() == \"lstm\":\n",
    "        config = LSTMConfig(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim, n_classes=n_classes, dropout=dropout)\n",
    "        return TextClassificationRawLSTM(config)\n",
    "\n",
    "    raise ValueError(\"MODEL_TYPE deve ser 'rnn' ou 'lstm'.\")\n",
    "\n",
    "print(f\"Arquitetura selecionada: {MODEL_TYPE.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f3a59",
   "metadata": {},
   "source": [
    "## 2) Pipeline de Dados\n",
    "\n",
    "Utilizamos o dataset do desafio e um tokenizer apenas para mapeamento de vocabul√°rio e padding din√¢mico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data Pipeline (Emotion CSV)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def resolve_train_csv(root: Path) -> Path:\n",
    "    kaggle_path = Path(\"/kaggle/input/datasets/pablohenriquelemes/emotion-classification-uc14/train_large.csv\")\n",
    "    local_candidates = [\n",
    "        root / \"data\" / \"kaggle_emotion_classification\" / \"train_large.csv\",\n",
    "        root / \"train_large.csv\",\n",
    "    ]\n",
    "    if kaggle_path.exists():\n",
    "        return kaggle_path\n",
    "    for candidate in local_candidates:\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"N√£o foi poss√≠vel localizar train_large.csv.\")\n",
    "\n",
    "project_root = Path.cwd()\n",
    "if not (project_root / \"data\").exists() and (project_root.parent / \"data\").exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "train_csv = resolve_train_csv(project_root)\n",
    "print(f\"Arquivo de treino: {train_csv}\")\n",
    "\n",
    "raw_dataset = load_dataset(\"csv\", data_files={\"train\": str(train_csv)})[\"train\"]\n",
    "raw_dataset = raw_dataset.class_encode_column(\"label\")\n",
    "dataset = raw_dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"label\")\n",
    "\n",
    "num_labels = raw_dataset.features[\"label\"].num_classes\n",
    "print(f\"Total de classes: {num_labels}\")\n",
    "\n",
    "train_labels_np = np.array(dataset[\"train\"][\"label\"])\n",
    "val_labels_np = np.array(dataset[\"test\"][\"label\"])\n",
    "train_counts = np.bincount(train_labels_np, minlength=num_labels)\n",
    "val_counts = np.bincount(val_labels_np, minlength=num_labels)\n",
    "print(\"Distribui√ß√£o de classes (train):\", train_counts.tolist())\n",
    "print(\"Distribui√ß√£o de classes (val):  \", val_counts.tolist())\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False, max_length=128)\n",
    "\n",
    "columns_to_remove = [col for col in dataset[\"train\"].column_names if col != \"label\"]\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd19634",
   "metadata": {},
   "source": [
    "## 3) Treinamento e M√©tricas\n",
    "\n",
    "M√©tricas monitoradas por √©poca: Training Loss, Validation Loss, Accuracy, Log Loss, Precision, Recall e F1 (weighted e macro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Training Setup (Manual RNN/LSTM)\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "is_mps = torch.backends.mps.is_available()\n",
    "train_batch_size = 64\n",
    "\n",
    "model = build_manual_model(\n",
    "    model_type=MODEL_TYPE,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_classes=num_labels,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128,\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "train_counts = np.bincount(np.array(tokenized_datasets[\"train\"][\"label\"]), minlength=num_labels)\n",
    "class_weights_np = train_counts.sum() / np.maximum(train_counts, 1)\n",
    "class_weights_np = class_weights_np / class_weights_np.mean()\n",
    "class_weights = torch.tensor(class_weights_np, dtype=torch.float)\n",
    "print(\"Class weights:\", class_weights_np.round(4).tolist())\n",
    "\n",
    "def multiclass_log_loss(logits, labels, eps=1e-15):\n",
    "    probs = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "    probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "    probs = np.clip(probs, eps, 1.0 - eps)\n",
    "    one_hot = np.eye(probs.shape[1])[labels]\n",
    "    return float(-(one_hot * np.log(probs)).sum(axis=1).mean())\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    ll = multiclass_log_loss(logits, labels)\n",
    "\n",
    "    precision_weighted = precision_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"precision\"]\n",
    "    recall_weighted = recall_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"recall\"]\n",
    "    f1_weighted = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
    "\n",
    "    precision_macro = precision_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"]\n",
    "    recall_macro = recall_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"]\n",
    "    f1_macro = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"log_loss\": ll,\n",
    "        \"precision_weighted\": precision_weighted,\n",
    "        \"recall_weighted\": recall_weighted,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "    }\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self._cached_cw = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            if self._cached_cw is None or self._cached_cw.device != logits.device:\n",
    "                self._cached_cw = self.class_weights.to(logits.device)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self._cached_cw)\n",
    "        else:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = loss_fct(logits.view(-1, model.config.n_classes), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "output_dir = f\"./manual_{MODEL_TYPE}_emotion\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=max(32, train_batch_size),\n",
    "    num_train_epochs=5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_log_loss\",\n",
    "    greater_is_better=False,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=50,\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=False if is_mps else True,\n",
    "    remove_unused_columns=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "\n",
    "print(f\"Device profile -> CUDA: {is_cuda} | MPS: {is_mps}\")\n",
    "print(f\"‚úÖ Modelo manual criado: {MODEL_TYPE.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d19fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Start training\n",
    "\n",
    "trainer.model.to(device)\n",
    "print(f\"Treinando em: {device}\")\n",
    "\n",
    "if device.type == \"mps\" and hasattr(torch, \"mps\"):\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d33277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Save trained model\n",
    "\n",
    "save_directory = f\"./manual_{MODEL_TYPE}_emotion/final_checkpoint\"\n",
    "trainer.save_model(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(f\"Checkpoint salvo em: {save_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86969b3c",
   "metadata": {},
   "source": [
    "## 4) Avalia√ß√£o, Infer√™ncia e Submiss√£o Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e265ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load checkpoint treinado\n",
    "\n",
    "project_root = Path.cwd()\n",
    "if not (project_root / \"data\").exists() and (project_root.parent / \"data\").exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "candidate_dirs = [\n",
    "    project_root / f\"manual_{MODEL_TYPE}_emotion\" / \"final_checkpoint\",\n",
    "    project_root / \"notebooks\" / f\"manual_{MODEL_TYPE}_emotion\" / \"final_checkpoint\",\n",
    "]\n",
    "\n",
    "save_directory = None\n",
    "for c in candidate_dirs:\n",
    "    if c.exists():\n",
    "        save_directory = str(c)\n",
    "        break\n",
    "\n",
    "if save_directory is None:\n",
    "    raise FileNotFoundError(\"Nenhum checkpoint final encontrado para o modelo manual selecionado.\")\n",
    "\n",
    "print(f\"Carregando checkpoint de: {save_directory}\")\n",
    "if MODEL_TYPE.lower() == \"rnn\":\n",
    "    loaded_model = TextClassificationRawRNN.from_pretrained(save_directory)\n",
    "else:\n",
    "    loaded_model = TextClassificationRawLSTM.from_pretrained(save_directory)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Relat√≥rio completo de valida√ß√£o\n",
    "\n",
    "eval_trainer = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=TrainingArguments(output_dir=\"./eval_output_manual\", report_to=\"none\"),\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=loaded_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metrics = eval_trainer.evaluate()\n",
    "\n",
    "print(\"\\nüìä RELAT√ìRIO DE VALIDA√á√ÉO (MODELO MANUAL)\")\n",
    "print(f\"Validation Loss        : {metrics['eval_loss']:.5f}\")\n",
    "print(f\"Validation Log Loss    : {metrics['eval_log_loss']:.5f}\")\n",
    "print(f\"Validation Accuracy    : {metrics['eval_accuracy']:.2%}\")\n",
    "print(f\"Precision (weighted)   : {metrics['eval_precision_weighted']:.2%}\")\n",
    "print(f\"Recall (weighted)      : {metrics['eval_recall_weighted']:.2%}\")\n",
    "print(f\"F1-score (weighted)    : {metrics['eval_f1_weighted']:.2%}\")\n",
    "print(f\"Precision (macro)      : {metrics['eval_precision_macro']:.2%}\")\n",
    "print(f\"Recall (macro)         : {metrics['eval_recall_macro']:.2%}\")\n",
    "print(f\"F1-score (macro)       : {metrics['eval_f1_macro']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16427a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Infer√™ncia em texto novo\n",
    "\n",
    "label_map = {0: \"Sadness\", 1: \"Joy\", 2: \"Love\", 3: \"Anger\", 4: \"Fear\", 5: \"Surprise\"}\n",
    "\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "def predict_emotion(text):\n",
    "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=False)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        predicted_class_id = torch.argmax(outputs[\"logits\"], dim=-1).item()\n",
    "\n",
    "    return label_map[predicted_class_id]\n",
    "\n",
    "sample_text = \"I am very proud of this IA project result.\"\n",
    "print(\"Texto:\", sample_text)\n",
    "print(\"Emo√ß√£o prevista:\", predict_emotion(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085479a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Gerar submiss√£o Kaggle\n",
    "\n",
    "def _resolve_test_csv(root: Path) -> Path:\n",
    "    kaggle_path = Path(\"/kaggle/input/datasets/pablohenriquelemes/emotion-classification-uc14/test.csv\")\n",
    "    local_candidates = [\n",
    "        root / \"data\" / \"kaggle_emotion_classification\" / \"test.csv\",\n",
    "        root / \"test.csv\",\n",
    "    ]\n",
    "    if kaggle_path.exists():\n",
    "        return kaggle_path\n",
    "    for candidate in local_candidates:\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Arquivo de teste n√£o encontrado (test.csv).\")\n",
    "\n",
    "def _load_test_rows(csv_path: Path):\n",
    "    with csv_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if not reader.fieldnames:\n",
    "            raise ValueError(\"CSV de teste sem cabe√ßalho.\")\n",
    "\n",
    "        fieldnames = [name.strip() for name in reader.fieldnames]\n",
    "        id_field = \"id\" if \"id\" in fieldnames else None\n",
    "        text_field = \"text\" if \"text\" in fieldnames else None\n",
    "\n",
    "        if text_field is None:\n",
    "            non_id_fields = [name for name in fieldnames if name != \"id\"]\n",
    "            if not non_id_fields:\n",
    "                raise ValueError(\"CSV de teste precisa de uma coluna de texto.\")\n",
    "            text_field = non_id_fields[0]\n",
    "\n",
    "        ids, texts = [], []\n",
    "        for idx, row in enumerate(reader):\n",
    "            row_id = row.get(id_field) if id_field else None\n",
    "            ids.append(row_id if row_id is not None else str(idx))\n",
    "            texts.append(row.get(text_field, \"\"))\n",
    "\n",
    "    return ids, texts\n",
    "\n",
    "def _batched(items, batch_size):\n",
    "    for start in range(0, len(items), batch_size):\n",
    "        yield items[start:start + batch_size]\n",
    "\n",
    "test_csv = _resolve_test_csv(project_root)\n",
    "ids, texts = _load_test_rows(test_csv)\n",
    "\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "all_probs = []\n",
    "batch_size = 128\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_texts in _batched(texts, batch_size):\n",
    "        inputs = loaded_tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = loaded_model(**inputs)\n",
    "        probs = torch.softmax(outputs[\"logits\"], dim=-1).cpu().numpy()\n",
    "        all_probs.extend(probs)\n",
    "\n",
    "all_probs = np.asarray(all_probs, dtype=np.float64)\n",
    "if np.isnan(all_probs).any() or np.isinf(all_probs).any():\n",
    "    raise ValueError(\"Foram encontrados NaN/Inf nas probabilidades.\")\n",
    "\n",
    "row_sums = all_probs.sum(axis=1)\n",
    "if not np.allclose(row_sums, 1.0, atol=1e-6):\n",
    "    raise ValueError(\"As probabilidades n√£o somam 1.0 em todas as linhas.\")\n",
    "\n",
    "expected_num_classes = loaded_model.config.n_classes\n",
    "submission_class_columns = [f\"prob_{i}\" for i in range(expected_num_classes)]\n",
    "\n",
    "submission_path = project_root / f\"submission_manual_{MODEL_TYPE}.csv\"\n",
    "with submission_path.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\"] + submission_class_columns)\n",
    "    for row_id, prob_row in zip(ids, all_probs):\n",
    "        writer.writerow([row_id] + [f\"{p:.8f}\" for p in prob_row])\n",
    "\n",
    "print(f\"Arquivo de submiss√£o gerado: {submission_path}\")\n",
    "print(f\"Total de linhas: {len(all_probs)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
